---
layout: post
title:  Multi Layer Perception And Back Propagation Step By Step in Bangla 
categories: [tutorial]
tags:
- Machine Learning
- Neural Network
- Multi Layer Perception
- Back Propagation
---



যারা স্র্যাচ থেকে (Multi layer Perception) মাল্টিলেয়ার পারসেপশন ও Back propagation কিভাবে কাজ করে বুঝার চেষ্টা করছে আশা করি তাদের কাজ এ দিবে এই লেখা। লেখার মধ্যে ভুল ভ্রান্তি হয়ে থাকলে অভিজ্ঞদের ঠিক করে দেয়ার জন্য উৎসাহিত করছি।  সহজে বোঝার জন্য এখানে XOR এর ডাটাসেট নিব । তাহলে ইনপুট এবং আউটপুট দেখা যাক।
                                                      $$Input$$

$$
X=
\begin{bmatrix}
   0 & 0&1 \\
   0& 1&1\\
   1&0&1\\
   1&1&1\\
\end{bmatrix}
$$

  $$Output$$

$$
y=
\begin{bmatrix}
   0 \\
   1\\
   1\\
   0\\
\end{bmatrix}
$$

আমাদের নিউরাল নেটওয়ার্ক এ মোট ৩ টি লেয়ার থাকবে। একটি ইনপুট লেয়ার, মাঝখানে হিডেন লেয়ার এবং শেষ এ আউটপুট লেয়ার।

![3 layer Neuron](https://3.bp.blogspot.com/-dYynvY_c_2g/WxEjIuoptkI/AAAAAAAAAa8/rz1yV6syRXck0NF1bF5tSzxhR4nQQFALACLcBGAs/s1600/nn.JPG)


প্রথমে আমরা র‍্যান্ডম  Weight নিব । ইনপুট লেয়ার এর সাথে হিডেন লেয়ার এর Weight গুলো কে w1 ম্যাট্রিক্স এ রাখব। হিডেন লেয়ার এর সাথে আউটপুট লেয়ার এর Weight গুলো কে w2 ম্যাট্রিক্স এ রাখব।

w1 ম্যাট্রিক্স হবে 3×2 সাইজ এর । কারন ইনপুট নোড ৩ টি এবং হিডেন নোড ২ টি। 
$$w1= 
\begin{bmatrix}
   0.39 & 0.13 \\
   0.49& 0.54\\
   0.27&0.11
\end{bmatrix}
 $$
w2  ম্যাট্রিক্স হবে 2×1 সাইজ এর । কারন হিডেন নোড  ২ টি এবং  আউটপুট নোড ১ টি 
$$w2= 
\begin{bmatrix}
   0.57724465  \\
   0.17249133\\
\end{bmatrix}
 $$


সবার প্রথমে আমরা Weighted Sum বের করব । Weighted Sum কে আমরা নিচের মত করে লিখতে পারি। $z_{h}$  কে আমরা Weighted Sum ধরলাম ।
              $$z_{h}=\sum_{i=1}^Xx_{i}w_{i}$$
 
 এই  $z_{h}$ কে পাওয়া যাবে $X$ ম্যাট্রিক্স এবং w1 ম্যাট্রিক্স এর গুন করার মাধ্যমে।
$$
z_{h}=
\begin{bmatrix}
   0 & 0&1 \\
   0& 1&1\\
   1&0&1\\
   1&1&1\\
\end{bmatrix}×
\begin{bmatrix}
   0.39 & 0.13 \\
   0.49& 0.54\\
   0.27&0.11
\end{bmatrix}=
\begin{bmatrix}
   1.16089356 & 0.78996619\\
 0.66854822 & 0.24032131\\
 0.76767187&  0.65979681\\
 0.27532653  &0.11015193
\end{bmatrix}
$$
 $z_{h}$    হল হিডেন লেয়ার  এর মান। কিন্তু  এই মান 0 এবং 1 এর মধ্যে রাখার জন্য আমরা $z_{h}$ কে সিগময়েড ফাংশন এ পাস করে $a_{h}$ বের করব। সিগময়েড ফাংশন হল
 $$sigmoid=\frac{1}{1+e^{-x}} $$
 তাহলে আমরা আমাদের ফাইনাল হিডেন লেয়ার $h_{1}$ এবং $h_{2}$ এর মান $a_{h}$ পেয়ে গেলাম। সিগময়েড ফাংশন এ পাস করার কারনে সব মানগুলো  0 এবং  1 এর মধ্যেই আছে।
 $$a_{h}=\frac{1}{1+e^{-z_{h}}} $$
$$
a_{h}=
\begin{bmatrix}
    0.76149504 & 0.68782407\\
 0.66117801 & 0.55979283\\
  0.68301706 & 0.65921474\\
  0.56840009 & 0.52751017
\end{bmatrix}
$$
এরপর আমরা হিডেন লেয়ার এর মান $a_{h}$ এবং Weight ম্যাট্রিক্স w2 এর মাধ্যমে আউটপুট লেয়ার এর Weighted Sum বের করব।
$$
z_{o}=
\begin{bmatrix}
  0.76149504 & 0.68782407\\
 0.66117801 & 0.55979283\\
  0.68301706 & 0.65921474\\
  0.56840009 & 0.52751017
\end{bmatrix}×
\begin{bmatrix}
   0.57724465 \\
   0.17249133
\end{bmatrix}=
\begin{bmatrix}
  0.55821263]\\
 0.47822088]\\
 0.50797677]\\
 0.41909684
\end{bmatrix}
$$
এই মান 0 এবং 1 এর মধ্যে রাখার জন্য আমরা $z_{o}$ কে সিগময়েড ফাংশন এ পাস করে $a_{o}$ বের করব। 
$$a_{o}=\frac{1}{1+e^{-z_{o}}} $$
$$
a_{o}=
\begin{bmatrix}
  0.63603888\\
 0.61732767\\
 0.62433206\\
  0.60326711
\end{bmatrix}
$$
কিন্তু এটা আমাদের ফাইনাল আউটপুট নয়। আমরা এই আউটপুট থেকে এই আসল আউটপুট এর ডিফারেন্স নিয়ে এরর Error বের করব ।
$$E_{a_{o}}=\frac{1}{2}×(a_{o}-y)^{2}$$

$$
E_{a_{o}}=
\begin{bmatrix}
  0.20227273\\
  0.07321906\\
  0.0705632\\
  0.1819656 
\end{bmatrix}
$$

## Back Propagation
এখন শুরু হবে Back Propagation । এর মানে হল আমরা যে এরর পেয়েছি সেটা কে কমিয়ে আনার জন্য আমরা বারবার আউটপুট লেয়ার থেকে ব্যাক করে  w1 ওবং w2 ম্যাট্রিক্স এর  Weight গুলো বার বার আপডেট করব এবং এরর ক্যালকুলেট করব। এভাবে এক সময় আমরা এরর অনেক কমিয়ে আসল আউটপুট এর মত মান প্রেডিক্ট করতে পারব। 
তাহলে প্রথমে আমরা বের করব w2 এর weight গুলোতে কি পরিমান পরিবর্তন করলে আমাদের এরর কমে যাবে। $\frac{dE_{a_{o}}}{dw2}$ এর যা মান পাব তা আমরা w2 থেকে বাদ দিব। এটি মূলত $E_{a_{o}}$ এর  partial derivative  w2 এর সাপেক্ষে। তাহলে ক্যালকুলাসের Chain Rule থেকে পাই,
$$\frac{dE_{a_{o}}}{dw2}=\frac{dE_{a_{o}}}{da_{o}}×\frac{da_{o}}{dz_{o}}×\frac{dz_{o}}{dw2}$$

এখন প্রথমে $\frac{dE_{a_{o}}}{da_{o}}$ এর মান বের করব।

$$\frac{dE_{a_{o}}}{da_{o}}=\frac{d}{da_{o}}(\frac{1}{2}×(a_{o}-y)^{2})$$
$$\frac{dE_{a_{o}}}{da_{o}}=\frac{d}{da_{o}}(\frac{1}{2}×2(a_{o}-y))=a_{o}-y$$
$$
\frac{dE_{a_{o}}}{da_{o}}=
\begin{bmatrix}
  0.63603888\\
 0.61732767\\
 0.62433206\\
  0.60326711
\end{bmatrix}-
\begin{bmatrix}
   0 \\
   1\\
   1\\
   0\\
\end{bmatrix}
$$
$$
\frac{dE_{a_{o}}}{da_{o}}=
\begin{bmatrix}
 0.63603888\\
 -0.38267233\\
 -0.37566794\\
 0.60326711
 \end{bmatrix}
$$
এরপর আমরা $\frac{d{a_{o}}}{dz_{o}}$ বের করব। তাহলে,
$$\frac{d{a_{o}}}{dz_{o}}=\frac{d}{dz_{o}}(a_{o})$$
$a_{o}$ আগে বের করা হয়েছে 
$$a_{o}=\frac{1}{1+e^{-z_{o}}} $$

$$\frac{d{a_{o}}}{dz_{o}}=\frac{d}{dz_{o}}(\frac{1}{1+e^{-z_{o}}})$$
চেনা চেনা লাগছে? হ্যা HSC তে ক্যালকুলাস বই এর এই ম্যাথ টা খুব ইম্পর্টেন্ট ছিল। ভার্সিটির প্রথম সেমিষ্টার এ অনেকে করেছে। যাই হোক আগের ম্যাথ গুলো সহজ ছিল। এই ম্যাথ টা সলভ করে একটি রুল পাব যেই রুল অনেক বই/ টিউটোরিয়াল এ ডিরেক্ট দিয়ে দিয়েছে।  
$$\frac{d{a_{o}}}{dz_{o}}=\frac{d}{dz_{o}}({(1+e^{-z_{o}})^{-1}})$$

$$\frac{d{a_{o}}}{dz_{o}}=-1×(1+e^{-z_{o}})^{-2}×(-e^{-z_{o}})$$

$$\frac{d{a_{o}}}{dz_{o}}=\frac{e^{-z_{o}}}{(1+e^{-z_{o}})^{-2}}$$

$$\frac{d{a_{o}}}{dz_{o}}=\frac{1}{1+e^{-z_{o}}}×\frac{e^{-z_{o}}}{(1+e^{-z_{o}})}$$

$$\frac{d{a_{o}}}{dz_{o}}=\frac{1}{1+e^{-z_{o}}}×\frac{1+e^{-z_{o}}-1}{(1+e^{-z_{o}})}$$

$$\frac{d{a_{o}}}{dz_{o}}=\frac{1}{1+e^{-z_{o}}}×[\frac{1+e^{-z_{o}}}
{(1+e^{-z_{o}})}-\frac{1}{1+e^{-z_{o}}}]$$

$$\frac{d{a_{o}}}{dz_{o}}=\frac{1}{1+e^{-z_{o}}}×(1-\frac{1}{1+e^{-z_{o}}})$$
$\frac{1}{1+e^{-z_{o}}}$ হল $a_{o}$ এর মান। এখন $a_{o}$  বসিয়ে দিয়ে পাই,
$$\frac{d{a_{o}}}{dz_{o}}=a_{o}×(1-a_{o})$$

$$\frac{d{a_{o}}}{dz_{o}}=
\begin{bmatrix}
 0.63603888\\
 -0.38267233\\
 -0.37566794\\
 0.60326711
 \end{bmatrix}×(1-
 \begin{bmatrix}
 0.63603888\\
 -0.38267233\\
 -0.37566794\\
 0.60326711
 \end{bmatrix})=
 \begin{bmatrix}
  0.23149342\\
  0.23623422\\
  0.23454154\\
  0.23933590 
  \end{bmatrix}
$$

এটাই হল সেই সূত্র। এরপর বাকি আছে $\frac{d{z_{o}}}{dw2}$ 
$$\frac{d{z_{o}}}{dw2}=\frac{d}{dw2}{(w_{7}×a_{h1}+w_{8}×a_{h2})}=a_{h}$$
$a_{h}$ আমরা প্রথমেই বের করেছিলাম
$$
a_{h}=
\begin{bmatrix}
    0.76149504 & 0.68782407\\
 0.66117801 & 0.55979283\\
  0.68301706 & 0.65921474\\
  0.56840009 & 0.52751017
\end{bmatrix}
$$
তাহলে আমরা Chain Rule এর  সব টার্ম বের করে ফেলেছি। এখন $\frac{dE_{a_{o}}}{dw2}$ বের করব টার্ম গুলোর মান বসিয়ে। তবে $\frac{da_{o}}{dz_{o}}$ ও $\frac{dE_{a_{o}}}{da_{o}}$  এলিমেন্ট ওয়াইজ গুন করব। প্রাপ্ত গুন ফল এর সাথে $\frac{dz_{o}}{dw2}$ এর ম্যাট্রিক্স গুন করব।
$$\frac{dE_{a_{o}}}{dw2}=\frac{dz_{o}}{dw2}×(\frac{da_{o}}{dz_{o}}*\frac{dE_{a_{o}}}{da_{o}})$$
$$st=
 \begin{bmatrix}
  0.23149342\\
  0.23623422\\
  0.23454154\\
  0.23933590 
  \end{bmatrix}*
\begin{bmatrix}
 0.63603888\\
 -0.38267233\\
 -0.37566794\\
 0.60326711
 \end{bmatrix}= 
  \begin{bmatrix}
  0.14723882\\
 -0.0904003 \\
 -0.08810974\\
 0.14438348
 \end{bmatrix}
$$
 $\frac{dz_{o}}{dw2}$ হল 4×2 ম্যাট্রিক্স , $st$ হল 4×1 ম্যাট্রিক্স। ম্যাট্রিক্স গুন এর জন্য ১ম ম্যাট্রিক্স এর কলাম এবং ২য় ম্যাট্রিক্স এর সারি সমান হতে হয়। 

তাই  $\frac{dz_{o}}{dw2}$ কে ট্রান্সপোজ ম্যাট্রিক্স এ কনভার্ট করে নিব। তাহলে, 

$$\frac{dE_{a_{o}}}{dw2}=
 \begin{bmatrix}
0.76149504   & 0.66117801 & 0.68301706 & 0.56840009\\ 
0.68782407 & 0.55979283& 0.65921474 & 0.52751017\\
\end{bmatrix}×st
$$
$$\frac{dE_{a_{o}}}{dw2}=
\begin{bmatrix}
0.07423807\\
 0.06874948
 \end{bmatrix}
$$
এখন আমরা  $w2$ এর Weight আপডেট করব। $w2$ থেকে  $\frac{dE_{a_{o}}}{dw2}$ বিয়োগ করে আমরা আপডেট করব। তবে দ্রুত Converge এর জন্য আমরা 
$\frac{dE_{a_{o}}}{dw2}$ কে $eta=3$ দ্বারা  গুন করে নিব।

$$w2=w2-eta*\frac{dE_{a_{o}}}{dw2}$$

$$
w2=
\begin{bmatrix}
   0.57724465  \\
   0.17249133\\
\end{bmatrix}-3*
\begin{bmatrix}
0.07423807\\
 0.06874948
 \end{bmatrix}
 $$
 
$$
w2=
\begin{bmatrix}
0.35453044\\
 -0.03375711
  \end{bmatrix}
  $$
  সুতরাং প্রথম ধাপ শেষ। এবার আমাদের আরও ব্যাক এ যেতে হবে $w1$ এর Weight আপডেট এর জন্য। তাহলে  Chain Rule অনুসারে,
  $$\frac{dE_{a_{o}}}{dw1}=\frac{dE_{a_{o}}}{da_{h}}×\frac{da_{h}}{dz_{h}}×\frac{dz_{h}}{dw1}$$
  অর্থাৎ $w1$ এর Weight এ কি পরিমান ডিফারেন্স আনলে আমাদের Error কমে যাবে। এখানে  $\frac{dE_{a_{o}}}{da_{h}}$ এর  $dE_{a_{o}}$ এর সাথে $da_{h}$ এর সরাসরি সম্পর্ক নাই তাই  $\frac{dE_{a_{o}}}{da_{h}}$  কেও Chain Rule অনুসারে ভেঙ্গে ফেলতে হবে।
 $$\frac{dE_{a_{o}}}{da_{h}}=\frac{dE_{a_{o}}}{dz_{o}}×\frac{d{z_{o}}}{da_{h}}$$
 
  $$\frac{dE_{a_{o}}}{da_{h}}=\frac{dE_{a_{o}}}{da_{o}}*\frac{d{a_{o}}}{dz_{o}}×\frac{d{z_{o}}}{da_{h}}$$
$\frac{dE_{a_{o}}}{da_{o}}×\frac{d{a_{o}}}{dz_{o}}$ এর মান আমরা আগে বের করেছি $st$ দিয়ে। 

তাহলে $\frac{d{z_{o}}}{da_{h}}$ এর মান বের করি,
$$\frac{d{z_{o}}}{da_{h}}=\frac{d}{da_{h}}{(w_{7}×a_{h1}+w_{8}×a_{h2})}=w2$$

$w2$ কে ট্রান্সপোজ ম্যাট্রিক্স এ কনভার্ট করে $st$ এর সাথে ম্যাট্রিক্স গুন করে পাই,
$$\frac{dE_{a_{o}}}{da_{h}}=
\begin{bmatrix}
  0.14723882\\
 -0.0904003 \\
 -0.08810974\\
 0.14438348
 \end{bmatrix}×
 \begin{bmatrix}
0.07423807 & 0.06874948
 \end{bmatrix}
$$
$$
\frac{dE_{a_{o}}}{da_{h}}=
\begin{bmatrix}
0.05220064 & -0.00497036\\
 -0.03204966 & 0.00305165\\
 -0.03123758 & 0.00297433\\
 0.05118834& -0.00487397
 \end{bmatrix}
 $$
 $$\frac{d{a_{h}}}{dz_{h}}=a_{h}×(1-a_{h})$$
 
  $$\frac{d{a_{h}}}{dz_{h}}=
  \begin{bmatrix}
  0.18162034 & 0.21472212\\
  0.22402165 & 0.24642482\\
 0.21650476 & 0.22465067\\
 0.24532143&  0.24924319
 \end{bmatrix}
  $$
  সবার শেষ এর টার্ম  $\frac{dz_{h}}{dw1}$ এর ক্ষেত্রে ,
  
$$\frac{dz_{h}}{dw1}=\frac{d}{dw1}{(w_{1}×x_{0}+w_{3}×x_{1}+w_{2}×x_{0}+w_{4}×x_{1}+...)}=X$$ 
এখন $\frac{dE_{a_{o}}}{da_{h}}$ এবং  $\frac{d{a_{h}}}{dz_{h}}$ এর এলিমেন্ট ওয়াইজ গুন হবে যা $fs$
$$fs=
\begin{bmatrix}
0.05220064 & -0.00497036\\
 -0.03204966 & 0.00305165\\
 -0.03123758 & 0.00297433\\
 0.05118834& -0.00487397
 \end{bmatrix}*
 \begin{bmatrix}
  0.18162034 & 0.21472212\\
  0.22402165 & 0.24642482\\
 0.21650476 & 0.22465067\\
 0.24532143&  0.24924319
 \end{bmatrix}
$$
$$fs=
\begin{bmatrix}
0.0094807 &-0.00106725\\
 -0.00717982 &0.000752 \\
 -0.00676309 &0.00066819\\
 0.0125576 &-0.0012148 
 \end{bmatrix}
$$
সবার শেষ এ $\frac{dz_{h}}{dw1}$ এর ট্রান্সপোজ ম্যাট্রিক্স এর সাথে  $fs$ এর ম্যাট্রিক্স গুন হবে,

$$\frac{dE_{a_{o}}}{da_{h}}=
\begin{bmatrix}
   0 & 0&1 &1 \\
   0& 1&0&1\\
   1&1&1&1\\
\end{bmatrix}×
\begin{bmatrix}
0.0094807 &-0.00106725\\
 -0.00717982 &0.000752 \\
 -0.00676309 &0.00066819\\
 0.0125576 &-0.0012148 
 \end{bmatrix}
$$
$$\frac{dE_{a_{o}}}{da_{h}}=
\begin{bmatrix}
0.00230088 &-0.00031524\\
 0.00271761 &-0.00039906\\
 0.00809539 &-0.00086186
 \end{bmatrix}
 $$
 $w1$ আপডেট করব $w1$ এর নিয়মে, 
 $$w1=w1-eta*\frac{dE_{a_{o}}}{dw1}$$

$$
w1=
\begin{bmatrix}
   0.39 & 0.13 \\
   0.49& 0.54\\
   0.27&0.11
\end{bmatrix}-3*
\begin{bmatrix}
0.00230088 &-0.00031524\\
 0.00271761 &-0.00039906\\
 0.00809539 &-0.00086186
 \end{bmatrix}
 $$
 $$
w1=
\begin{bmatrix}
0.38631905& 0.13111511\\
 0.4841925 &0.55084206\\
 0.25104035 &0.11273751
  \end{bmatrix}
  $$
 Back propagation এর কাজ শেষ । এখন আপডেটেড Weight $w1$ এবং $w2$ দিয়ে আবার আগের মত $a_{h}$ এবং $a_{o}$ করে দেখব আসল আউটপুট এর কাছাকাছি কি না। সাধারন্ত একবার Back propagation করে কাছাকাছি আসবে না। প্রায় ২০০০০-৩০০০০ বারের মত Back propagation করে আমরা একবারে কাছাকছি ভ্যালু পেয়ে যাব। নিচের কোড এ Back propagation এর ইমপ্লিমেন্টেশন ঠিক একইভাবে করা হয়েছে।

    
    import numpy as np
    
    X = np.array([
    [0, 0, 1],
    [0, 1, 1],
    [1, 0, 1],
    [1, 1, 1]])
    y = np.array([[0],
                  [1],
                  [1],
                  [0]])
                  
    #Weight Initialize             
    w1=np.random.random((3,2))
    w2=np.random.random((2,1))
    
    #sigmoid function   
    def sigmoid(x, deriv=False):
	  if(deriv):
	    return x*(1-x)
	  else:
	    return (1/(1+np.exp(-x)))
	 
	#Back propagation   
	for epoch in range(0,30000):
  
	  eta=0.5
	  #Weighted Sum

	  z_h=np.dot(X,w1)
	  a_h=sigmoid(z_h)
	  
	  # second weighted sum
	  z_o=np.dot(a_h,w2)
	  a_o=sigmoid(z_o)
	  #Error Calculate
	  Ea=(0.5*(np.power((a_o-y),2)))
	  #BackPropagation dE/dw2
	  #dE/da0
	  dEda0=(a_o-y)
	  #da0/dz0
	  da0dz0=sigmoid(a_o,deriv=True)
	  #dz0/dw
	  dz0dw=a_h
	  #dE/dw2
	  st=da0dz0*dEda0
	  dEdw2=np.dot(dz0dw.T,st)
	  #Weight update
	  w2=w2-(eta*dEdw2)
	  #hidden Layer dEdw1
	  #dE/dah
	  dEdah=np.dot(st,w2.T)
	  #dzh/dw
	  dzhdw=X
	  #dah/dzh
	  dahdzh=sigmoid(a_h,deriv=True)
	  fs=dEdah*dahdzh
	  dEdw1=np.dot(dzhdw.T,fs)
	  #Weight Update
	  w1=w1-(eta*dEdw1)
	 
	print(w1)
	print(w2)
	
    #testing final output
    test_a_h=sigmoid(np.dot(X,w1));
    prediction=sigmoid(np.dot(test_a_h,w2))
    print(prediction)
    
           
            
    


           
            
    

